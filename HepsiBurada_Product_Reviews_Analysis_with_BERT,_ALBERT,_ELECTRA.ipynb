{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading File"
      ],
      "metadata": {
        "id": "7oOBcRvNdF2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "botJIDL0c6_F"
      },
      "outputs": [],
      "source": [
        "def readFile(fileName):\n",
        "    file = open(fileName, encoding = 'utf-8')\n",
        "    rows = file.readlines() #read all lines\n",
        "    file.close()\n",
        "\n",
        "    file_ = []\n",
        "    for row in rows:\n",
        "        row = row.lower() #lower text\n",
        "        file_.append(str(satir).rstrip(\"\\n\"))\n",
        "\n",
        "    return file_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = readFile('hepsiS1.txt')\n",
        "stopwords = readFile('turkish_stopwordsV3.txt')"
      ],
      "metadata": {
        "id": "c0rluJs3duRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Count"
      ],
      "metadata": {
        "id": "v-tjelbsdMdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "total_word = []\n",
        "for txt in files:\n",
        "  text_tokens = word_tokenize(txt)\n",
        "  for token in text_tokens:\n",
        "    total_word.append(token)\n",
        "print(\"Toplam kelime say覺s覺: \" + str(len(total_word)))\n",
        "total_words = list(set(total_word))\n",
        "print(\"Unique kelime say覺s覺: \" + str(len(total_words)))"
      ],
      "metadata": {
        "id": "h1w00dHrdKz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating CSV files for training"
      ],
      "metadata": {
        "id": "jtQENXgKeOy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#create csv file\n",
        "with open('hepsiS1.csv', 'w', newline='',encoding='utf-8') as file:\n",
        "    fieldnames = ['labels', 'text']\n",
        "    #define dictionary writer\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    ind = 0\n",
        "    label = 0\n",
        "    #read row in files\n",
        "    for row in files:   \n",
        "      #tokenize sentence\n",
        "      text_tokens = word_tokenize(row)\n",
        "      #remove stopwords in sentence\n",
        "      tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
        "      #new sentence after processes\n",
        "      filtered_sentence = (\" \").join(tokens_without_sw)   \n",
        "      writer.writerow({'labels': label, 'text': row})\n",
        "      ind += 1\n",
        "      if ind == 13676: #change label according to dataset's index\n",
        "        label += 1"
      ],
      "metadata": {
        "id": "X1PcYngKeRtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data for Turkish text classification and Create Dataframe"
      ],
      "metadata": {
        "id": "cU4qf1kif8BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(\"hepsiS1.csv\")\n",
        "df.columns=[\"labels\",\"text\"]\n",
        "df.labels=pd.Categorical(df.labels)\n",
        "df['labels'] = pd.factorize(df.labels)[0]"
      ],
      "metadata": {
        "id": "tNwPTVBBf6Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "import torch,sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split a dataset into train and test sets\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2)"
      ],
      "metadata": {
        "id": "5yEoi_XJgMSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Language Models"
      ],
      "metadata": {
        "id": "yZTLuNKSgStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelb = ClassificationModel('bert', 'dbmdz/bert-base-turkish-cased', num_labels=2, \n",
        "                            args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3})\n",
        "\n",
        "modelb.train_model(train_df, acc=sklearn.metrics.accuracy_score)\n",
        "\n",
        "modele = ClassificationModel('electra', 'dbmdz/electra-base-turkish-cased-discriminator', num_labels=2, \n",
        "                            args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3})\n",
        "\n",
        "modele.train_model(train_df, acc=sklearn.metrics.accuracy_score)\n",
        "\n",
        "modela = ClassificationModel('albert', 'loodos/albert-base-turkish-uncased', num_labels=2, \n",
        "                            args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3})\n",
        "\n",
        "modela.train_model(train_df, acc=sklearn.metrics.accuracy_score)"
      ],
      "metadata": {
        "id": "uci1XhqSgUt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing BERT Language Model"
      ],
      "metadata": {
        "id": "ZebqxmuPgXn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = eval_df \n",
        "result, model_outputs, wrong_predictions = modelb.eval_model(test)\n",
        "predictions = model_outputs.argmax(axis=1)\n",
        "actuals = test.labels.values\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(actuals, predictions))"
      ],
      "metadata": {
        "id": "sq86v3CHgWoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence predict example"
      ],
      "metadata": {
        "id": "1K84FolPd8g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = test.iloc[10]['text']\n",
        "print(sample_text)\n",
        "model.predict([sample_text])\n",
        "test.iloc[10]['labels']"
      ],
      "metadata": {
        "id": "9SgE94CDgeg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}